{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFji9bO_eJMR"
      },
      "source": [
        "# Implémentez un modèle de scoring\n",
        "\n",
        "- **Projet 7 du parcours « Data Scientist » d’OpenClassrooms**\n",
        "- **Mark Creasey**\n",
        "\n",
        "## Étape 1 : Préparation du jeu de données et feature engineering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Compréhension du problème\n",
        "\n",
        "### 1.1.1 Problématique\n",
        "\n",
        "La société financière, nommée **\"Prêt à dépenser\"**, propose des crédits à la consommation pour des\n",
        "personnes ayant peu ou pas du tout d'historique de prêt.\n",
        "\n",
        "L’entreprise souhaite mettre en œuvre **un outil de “scoring crédit”** pour calculer la qu’un client\n",
        "rembourse son crédit, puis classifie la demande en crédit accordé ou refusé. Elle souhaite donc\n",
        "développer **un algorithme de classification** en s’appuyant sur des sources de données variées (données\n",
        "comportementales, données provenant d'autres institutions financières, etc.).\n",
        "\n",
        "### 1.1.2 Les données\n",
        "\n",
        "Voici [les données](https://www.kaggle.com/c/home-credit-default-risk/data) pour réaliser le\n",
        "dashboard. Pour plus de simplicité, vous pouvez les télécharger à\n",
        "[cette adresse](https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/Parcours_data_scientist/Projet+-+Impl%C3%A9menter+un+mod%C3%A8le+de+scoring/Projet+Mise+en+prod+-+home-credit-default-risk.zip).\n",
        "\n",
        "### 1.1.1 Mission\n",
        "\n",
        "- Sélectionner un kernel Kaggle pour faciliter la préparation des données nécessaires à l’élaboration du modèle de scoring.\n",
        "- Analyser ce kernel et l’adapter aux besoins de votre mission.\n",
        "\n",
        "Focalise sur :\n",
        "\n",
        "1. La construction d'un **modèle de scoring** qui donnera une prédiction sur la probabilité de faillite\n",
        "   d'un client de façon automatique.\n",
        "   - élaboration\n",
        "   - optimisation\n",
        "   - comprehension (interpretabilité)\n",
        "2. Construction d'un **dashboard interactif** qui montre avec transparence les décisions d’octroi de\n",
        "   crédit, à destination des gestionnaires de la relation client permettant d'interpréter les\n",
        "   prédictions faites par le modèle et d’améliorer la connaissance client des chargés de relation\n",
        "   client.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM6z7uUfebpT"
      },
      "source": [
        "## 1.2 Definition de l'environnement\n",
        "\n",
        "- `local` : Développement local (avec échantillon de 50 Mo de données)\n",
        "- `colab` : Google Colab\n",
        "- `kaggle` : Kaggle Kernel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMeZBFf2eTfz",
        "outputId": "2657b460-a736-4de6-de5f-1bf9ceb1ea36"
      },
      "outputs": [],
      "source": [
        "ENV = 'local'\n",
        "\n",
        "if ENV == 'local':\n",
        "    # local development\n",
        "    DATA_FOLDER = '../data/raw'\n",
        "    OUT_FOLDER = '../data/out'\n",
        "    IMAGE_FOLDER = '../images/nettoyage'\n",
        "\n",
        "if ENV == 'colab':\n",
        "    # Colaboratory - uncomment les 2 lignes suivant pour connecter à votre drive\n",
        "    # from google.colab import drive\n",
        "    # drive.mount('/content/drive')\n",
        "    DATA_FOLDER = '/content/drive/MyDrive/data/OC7'\n",
        "    OUT_FOLDER = '/content/drive/MyDrive/data/OC7'\n",
        "    IMAGE_FOLDER = '/content/drive/MyDrive/images/OC7/nettoyage'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvnInn3XfWQ2"
      },
      "source": [
        "## 1.3 Fichiers de données\n",
        "\n",
        "1. Les données en format CSV (>700Mb compactés) sont à télecharger de ce lien:\n",
        "\n",
        "- https://www.kaggle.com/c/home-credit-default-risk/data\n",
        "- Pour plus de simplicité, vous pouvez les télécharger à [cette adresse.](https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/Parcours_data_scientist/Projet+-+Impl%C3%A9menter+un+mod%C3%A8le+de+scoring/Projet+Mise+en+prod+-+home-credit-default-risk.zip)\n",
        "\n",
        "2.  Placer le fichier compacté (**.zip**) dans le **DATA_FOLDER** défini ci-dessous\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELwbVYvAgSMU"
      },
      "source": [
        "### Noms des fichiers de données (identique pour nettoyage et l'analyse exploratoire)\n",
        "\n",
        "- Le grand fichier zip des données doit être placé dans `DATA_FOLDER` au préalable\n",
        "- Tous les autres fichiers de données sont téléchargés ou crées pendant le nettoyage, puis enregistrés dans `OUT_FOLDER`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F_iJnVSEfKma"
      },
      "outputs": [],
      "source": [
        "# Données (DATA_FOLDER)\n",
        "ZIPPED_DATA_FILENAME = f'Projet+Mise+en+prod+-+home-credit-default-risk.zip'\n",
        "RAW_DATA_FILENAME = 'HomeCredit_columns_description.csv'\n",
        "SAMPLE_DATA_FILENAME = 'HomeCredit_columns_description.csv'\n",
        "\n",
        "\n",
        "# Données nettoyés (OUT_FOLDER)\n",
        "CLEAN_DATA_FILENAME = 'cleaned_data_scoring.csv'\n",
        "CLEAN_DATA_SAMPLE = 'cleaned_data_sample.csv'  # 100,000 registres\n",
        "SAMPLE_SIZE = 100000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMFxPWtagxDT"
      },
      "source": [
        "## 1.4 Requirements: Bibliothèques utilisées dans ce notebook\n",
        "\n",
        "Ce notebook marche a été testé en developpement local, sur Google Colab et Kaggle\n",
        "\n",
        "```txt\n",
        "# copy dans un fichier requirements.txt, puis\n",
        "# !pip install -r requirements.txt\n",
        "python>=3.7,<=3.9\n",
        "numpy>=1.19.5,<=1.21.2\n",
        "pandas>=1.1.5,<=1.3.4\n",
        "matplotlib>=3.2.2,<=3.5.0\n",
        "seaborn==0.11.2\n",
        "scikit-learn>=1.0.1\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pIP-XhqSgyC_"
      },
      "outputs": [],
      "source": [
        "# Decommentarise la ligne suivant si vous ne voulez pas changer vos versions existants\n",
        "# !pip install numpy pandas matplotlib seaborn scipy sklearn missingno requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8kXrZGCg6h1"
      },
      "source": [
        "## 1.5 Import dependencies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPt54cG5g7Up"
      },
      "source": [
        "### 1.5.1 Import des bibliothèques utilisées par ce notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XtukhHGIg-Ag"
      },
      "outputs": [],
      "source": [
        "# suppress furture warnings de pandas 1.3.0\n",
        "from contextlib import contextmanager\n",
        "import time\n",
        "import gc\n",
        "import sklearn\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import warnings\n",
        "import platform\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhE8BZ1ihG2I"
      },
      "source": [
        "### 1.5.2 Liste des versions des bibliothèques utilisées\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJXh67-FhIwI",
        "outputId": "cf659d51-6099-46fb-c9fb-56d2067fe4a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python version = 3.7.0\n",
            "versions des bibliothèques utilisées:\n",
            "sklearn==1.0.2; seaborn==0.11.2; pandas==1.1.5; numpy==1.21.5; platform==1.0.8\n"
          ]
        }
      ],
      "source": [
        "print(f'python version = {platform.python_version()}')\n",
        "print('versions des bibliothèques utilisées:')\n",
        "print('; '.join(f'{m.__name__}=={m.__version__}' for m in globals(\n",
        ").values() if getattr(m, '__version__', None)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f04qBPghSsr"
      },
      "source": [
        "### 1.5.3 Configuration défauts d'affichage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mIoSzAqXhMEV"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', 200)  # pour afficher toutes les colonnes\n",
        "pd.set_option('display.max_rows', 10)  # pour afficher max 10 lignes\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_theme(style=\"white\", context=\"notebook\")\n",
        "sns.set_color_codes(\"pastel\")\n",
        "sns.set_palette(\"tab20\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bibliothèque personelle\n",
        "\n",
        "On utilise un nom non standard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import outils\n",
        "from outils.vis import to_png\n",
        "from outils.timed import timer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuration personelle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Enregistre parametres globals dans outils\n",
        "outils.vis.set_option('IMAGE_FOLDER', IMAGE_FOLDER)\n",
        "outils.vis.set_option('SAVE_IMAGES', True)\n",
        "\n",
        "if ENV != 'kaggle':\n",
        "    outils.io.os_make_dir(DATA_FOLDER)\n",
        "    outils.io.os_make_dir(OUT_FOLDER)\n",
        "\n",
        "outils.io.os_make_dir(IMAGE_FOLDER)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mhUYXKehkDI"
      },
      "source": [
        "# 2. Importation des données <a name=\"importation\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9UNgYuvhrRP"
      },
      "source": [
        "## 2.1 Configuration de l'environnement de travail\n",
        "\n",
        "### 2.1.1 Installation des bibliothèques nécessaires pour manipuler les données\n",
        "\n",
        "Pour plusieurs notebooks dans un dossier, enregistre une liste des bibliothèques dans un fichier `requirements.txt`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvsIMU0Ihglo",
        "outputId": "dee247f2-f59d-4167-c0fc-922e84b453fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "required modules: ['seaborn', 'requests', 'pandas', 'matplotlib', 'missingno', 'numpy']\n",
            "missing modules: []\n"
          ]
        }
      ],
      "source": [
        "outils.io.install_libraries({'numpy', 'pandas', 'matplotlib',\n",
        "                                'seaborn', 'requests', 'missingno'})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLoopUM9h3ly"
      },
      "source": [
        "## 2.2 Configuration de l'importation des données\n",
        "\n",
        "### 2.2.1 Choix de fichier à analyser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvhKfqv8hzTh",
        "outputId": "d48ef236-0027-4832-84c5-f3e7bc5cdb33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data file: ../data/raw/HomeCredit_columns_description.csv\n"
          ]
        }
      ],
      "source": [
        "DATA_FILENAME = SAMPLE_DATA_FILENAME if ENV == 'local' else RAW_DATA_FILENAME\n",
        "RAW_DATA = outils.io.os_path_join(DATA_FOLDER, DATA_FILENAME)\n",
        "DATA_ZIPPED = outils.io.os_path_join(DATA_FOLDER, ZIPPED_DATA_FILENAME)\n",
        "print(f'data file: {RAW_DATA}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWaoA47Rh6-o",
        "outputId": "3cf7287f-7fd0-4a2e-d571-3e65e8f5cf92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data CSV file exists (../data/raw/HomeCredit_columns_description.csv)\n"
          ]
        }
      ],
      "source": [
        "def unzip_data_si_besoin(env=ENV):\n",
        "    \"\"\"procedure pour unzip sur Google Drive via Google Colab\"\"\"\n",
        "    if os.path.exists(RAW_DATA):\n",
        "        print(f'data CSV file exists ({RAW_DATA})')\n",
        "    else:\n",
        "        print(f'data CSV file does not exist ({RAW_DATA})')\n",
        "        if env == 'colab' and os.path.exists(DATA_ZIPPED):\n",
        "            # uncomment les 3 lignes suivants\n",
        "            print(f'unzipping {DATA_ZIPPED}')\n",
        "            !unzip {DATA_ZIPPED} - d {DATA_FOLDER}\n",
        "            print(f'{DATA_ZIPPED} has been unzipped')\n",
        "            if os.path.exists(RAW_DATA):\n",
        "                print(f'data CSV file now exists ({RAW_DATA})')\n",
        "        else:\n",
        "            print(f'zipped data does not exist ({DATA_ZIPPED})')\n",
        "\n",
        "\n",
        "unzip_data_si_besoin(ENV)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWAejb0fi55g"
      },
      "source": [
        "### 2.2.2 Information sur le fichier (taille, type, nb. registres, champs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE_7aupwiqoS",
        "outputId": "4ee3d60b-77eb-4139-ff14-8b8954c885e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "file : application_test.csv : size=25.34 Mo\n",
            "file : application_train.csv : size=158.44 Mo\n",
            "file : bureau.csv : size=162.14 Mo\n",
            "file : bureau_balance.csv : size=358.19 Mo\n",
            "file : credit_card_balance.csv : size=404.91 Mo\n",
            "file : HomeCredit_columns_description.csv : size=36.51 ko\n",
            "file : installments_payments.csv : size=689.62 Mo\n",
            "file : POS_CASH_balance.csv : size=374.51 Mo\n",
            "file : previous_application.csv : size=386.21 Mo\n",
            "file : sample_submission.csv : size=523.63 ko\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for filename in os.listdir(DATA_FOLDER):\n",
        "    if filename.lower().endswith('csv'):\n",
        "        filepath = f'{DATA_FOLDER}/{filename}'\n",
        "        print(f'file : {filename} : size={outils.io.get_filesize(filepath)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAz4r0D1l_wE"
      },
      "source": [
        "## 2.3 Data schema\n",
        "\n",
        "Le schema et description des données est fourni sur le lien:\n",
        "\n",
        "- https://www.kaggle.com/competitions/home-credit-default-risk/data\n",
        "\n",
        "<p><img title=\"\" alt=\"Data\" src=\"img/home_credit.png\"></p>\n",
        "<em>Original source : https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png<em>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxQnAcG75KaL"
      },
      "source": [
        "### 2.3.1 Description des champs\n",
        "\n",
        "- details sur <https://www.kaggle.com/competitions/home-credit-default-risk/data>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AIWoT8Ze5HFH"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Table</th>\n",
              "      <th>Row</th>\n",
              "      <th>Description</th>\n",
              "      <th>Special</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>application_{train|test}.csv</td>\n",
              "      <td>SK_ID_CURR</td>\n",
              "      <td>ID of loan in our sample</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>application_{train|test}.csv</td>\n",
              "      <td>TARGET</td>\n",
              "      <td>Target variable (1 - client with payment difficulties: he/she had late payment more than X days ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>application_{train|test}.csv</td>\n",
              "      <td>NAME_CONTRACT_TYPE</td>\n",
              "      <td>Identification if loan is cash or revolving</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>application_{train|test}.csv</td>\n",
              "      <td>CODE_GENDER</td>\n",
              "      <td>Gender of the client</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>application_{train|test}.csv</td>\n",
              "      <td>FLAG_OWN_CAR</td>\n",
              "      <td>Flag if the client owns a car</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                         Table                 Row  \\\n",
              "0           1  application_{train|test}.csv          SK_ID_CURR   \n",
              "1           2  application_{train|test}.csv              TARGET   \n",
              "2           5  application_{train|test}.csv  NAME_CONTRACT_TYPE   \n",
              "3           6  application_{train|test}.csv         CODE_GENDER   \n",
              "4           7  application_{train|test}.csv        FLAG_OWN_CAR   \n",
              "\n",
              "                                                                                           Description  \\\n",
              "0                                                                             ID of loan in our sample   \n",
              "1  Target variable (1 - client with payment difficulties: he/she had late payment more than X days ...   \n",
              "2                                                          Identification if loan is cash or revolving   \n",
              "3                                                                                 Gender of the client   \n",
              "4                                                                        Flag if the client owns a car   \n",
              "\n",
              "  Special  \n",
              "0     NaN  \n",
              "1     NaN  \n",
              "2     NaN  \n",
              "3     NaN  \n",
              "4     NaN  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def field_descriptions(data_dir=DATA_FOLDER, num_rows=1000):\n",
        "    df = pd.read_csv(f'{data_dir}/HomeCredit_columns_description.csv',\n",
        "                     nrows=num_rows, encoding='unicode_escape')\n",
        "    return df\n",
        "\n",
        "\n",
        "field_descriptions().head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Join des tables\n",
        "\n",
        "Comme indiqué dans la cahier de charges, on sélectionne un kernel Kaggle pour faciliter la préparation des données nécessaires à l’élaboration du modèle de scoring.\n",
        "\n",
        "Les procédures de fusion des tables ci-dessous sont basés sur le pipeline de feature engineering présenté dans :\n",
        "\n",
        "- <https://www.kaggle.com/code/jsaguiar/lightgbm-with-simple-features/script>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGl_W-mgp1BN"
      },
      "source": [
        "### One-hot encoding for categorical columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D9OiYAGhp1Ml"
      },
      "outputs": [],
      "source": [
        "# One-hot encoding for categorical columns with get_dummies\n",
        "def one_hot_encoder(df, nan_as_category=True):\n",
        "    original_columns = list(df.columns)\n",
        "    categorical_columns = [\n",
        "        col for col in df.columns if df[col].dtype == 'object']\n",
        "    df = pd.get_dummies(df, columns=categorical_columns,\n",
        "                        dummy_na=nan_as_category)\n",
        "    new_columns = [c for c in df.columns if c not in original_columns]\n",
        "    return df, new_columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocess application_train.csv\n",
        "\n",
        "#### [application\\_{train|test}.csv](https://www.kaggle.com/competitions/home-credit-default-risk/data)\n",
        "\n",
        "This is the main table, broken into two files for Train (with TARGET) and Test (without TARGET).\n",
        "\n",
        "Static data for all applications. One row represents one loan in our data sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def application_train(data_dir='data', num_rows=None, nan_as_category=False):\n",
        "    # Read data\n",
        "    print(f'application_train, data_dir = {data_dir}')\n",
        "    df = pd.read_csv(f'{data_dir}/application_train.csv', nrows=num_rows)\n",
        "    print(f'Train samples: {len(df)}')\n",
        "\n",
        "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
        "    df = df[df['CODE_GENDER'] != 'XNA']\n",
        "\n",
        "    # Categorical features with Binary encode (0 or 1; two categories)\n",
        "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
        "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
        "    # Categorical features with One-Hot encode\n",
        "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
        "\n",
        "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
        "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
        "    # Some simple new features (percentages)\n",
        "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
        "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
        "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
        "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
        "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
        "    gc.collect()\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp3pFMJd1s-8"
      },
      "source": [
        "### Preprocess bureau.csv and bureau_balance.csv\n",
        "\n",
        "#### bureau.csv\n",
        "\n",
        "All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample).\n",
        "\n",
        "For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.\n",
        "\n",
        "#### bureau_balance.csv\n",
        "\n",
        "Monthly balances of previous credits in Credit Bureau.\n",
        "\n",
        "This table has one row for each month of history of every previous credit reported to Credit Bureau – i.e the table has\n",
        "`(#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "s5XZtAG41tKv"
      },
      "outputs": [],
      "source": [
        "# Preprocess bureau.csv and bureau_balance.csv\n",
        "def bureau_and_balance(data_dir='data', num_rows=None, nan_as_category=True):\n",
        "    bureau = pd.read_csv(f'{data_dir}/bureau.csv', nrows=num_rows)\n",
        "    bb = pd.read_csv(f'{data_dir}/bureau_balance.csv', nrows=num_rows)\n",
        "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
        "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
        "\n",
        "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
        "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
        "    for col in bb_cat:\n",
        "        bb_aggregations[col] = ['mean']\n",
        "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
        "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper()\n",
        "                              for e in bb_agg.columns.tolist()])\n",
        "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
        "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace=True)\n",
        "    del bb, bb_agg\n",
        "    gc.collect()\n",
        "\n",
        "    # Bureau and bureau_balance numeric features\n",
        "    num_aggregations = {\n",
        "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
        "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
        "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
        "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
        "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
        "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
        "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
        "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
        "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
        "        'AMT_ANNUITY': ['max', 'mean'],\n",
        "        'CNT_CREDIT_PROLONG': ['sum'],\n",
        "        'MONTHS_BALANCE_MIN': ['min'],\n",
        "        'MONTHS_BALANCE_MAX': ['max'],\n",
        "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
        "    }\n",
        "    # Bureau and bureau_balance categorical features\n",
        "    cat_aggregations = {}\n",
        "    for cat in bureau_cat:\n",
        "        cat_aggregations[cat] = ['mean']\n",
        "    for cat in bb_cat:\n",
        "        cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
        "\n",
        "    bureau_agg = bureau.groupby('SK_ID_CURR').agg(\n",
        "        {**num_aggregations, **cat_aggregations})\n",
        "    bureau_agg.columns = pd.Index(\n",
        "        ['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
        "    # Bureau: Active credits - using only numerical aggregations\n",
        "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
        "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
        "    active_agg.columns = pd.Index(\n",
        "        ['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
        "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
        "    del active, active_agg\n",
        "    gc.collect()\n",
        "    # Bureau: Closed credits - using only numerical aggregations\n",
        "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
        "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
        "    closed_agg.columns = pd.Index(\n",
        "        ['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
        "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
        "    del closed, closed_agg, bureau\n",
        "    gc.collect()\n",
        "    return bureau_agg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sDwgwuC1_Eu"
      },
      "source": [
        "### Preprocess previous_applications.csv\n",
        "\n",
        "#### previous_application.csv\n",
        "\n",
        "All previous applications for Home Credit loans of clients who have loans in our sample.\n",
        "\n",
        "There is one row for each previous application related to loans in our data sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "As3XXgPz1_Pa"
      },
      "outputs": [],
      "source": [
        "\n",
        "def previous_applications(data_dir='data', num_rows=None, nan_as_category=True):\n",
        "    prev = pd.read_csv(f'{data_dir}/previous_application.csv', nrows=num_rows)\n",
        "    prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n",
        "    # Days 365.243 values -> nan\n",
        "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n",
        "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n",
        "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n",
        "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n",
        "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n",
        "    # Add feature: value ask / value received percentage\n",
        "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
        "    # Previous applications numeric features\n",
        "    num_aggregations = {\n",
        "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
        "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
        "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
        "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
        "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
        "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
        "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
        "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
        "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
        "        'CNT_PAYMENT': ['mean', 'sum'],\n",
        "    }\n",
        "    # Previous applications categorical features\n",
        "    cat_aggregations = {}\n",
        "    for cat in cat_cols:\n",
        "        cat_aggregations[cat] = ['mean']\n",
        "\n",
        "    prev_agg = prev.groupby('SK_ID_CURR').agg(\n",
        "        {**num_aggregations, **cat_aggregations})\n",
        "    prev_agg.columns = pd.Index(\n",
        "        ['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
        "    # Previous Applications: Approved Applications - only numerical features\n",
        "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
        "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
        "    approved_agg.columns = pd.Index(\n",
        "        ['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
        "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
        "    # Previous Applications: Refused Applications - only numerical features\n",
        "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
        "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
        "    refused_agg.columns = pd.Index(\n",
        "        ['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
        "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
        "    del refused, refused_agg, approved, approved_agg, prev\n",
        "    gc.collect()\n",
        "    return prev_agg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nnrxj-O2IFq"
      },
      "source": [
        "### Preprocess POS_CASH_balance.csv\n",
        "\n",
        "#### POS_CASH_balance.csv\n",
        "\n",
        "Monthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit.\n",
        "\n",
        "This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has `(#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OeDdStMY2Ixf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def pos_cash(data_dir='data', num_rows=None, nan_as_category=True):\n",
        "    pos = pd.read_csv(f'{data_dir}/POS_CASH_balance.csv', nrows=num_rows)\n",
        "    pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n",
        "    # Features\n",
        "    aggregations = {\n",
        "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
        "        'SK_DPD': ['max', 'mean'],\n",
        "        'SK_DPD_DEF': ['max', 'mean']\n",
        "    }\n",
        "    for cat in cat_cols:\n",
        "        aggregations[cat] = ['mean']\n",
        "\n",
        "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
        "    pos_agg.columns = pd.Index(\n",
        "        ['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
        "    # Count pos cash accounts\n",
        "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
        "    del pos\n",
        "    gc.collect()\n",
        "    return pos_agg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V0nHJlK2Yrg"
      },
      "source": [
        "### Preprocess installments_payments.csv\n",
        "\n",
        "#### installments_payments.csv\n",
        "\n",
        "Repayment history for the previously disbursed credits in Home Credit related to the loans in our sample.\n",
        "\n",
        "There is a) one row for every payment that was made plus b) one row each for missed payment.\n",
        "\n",
        "One row is equivalent to one payment of one installment OR one installment corresponding to one payment of one previous Home Credit credit related to loans in our sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FtSh-YfQ2ZJj"
      },
      "outputs": [],
      "source": [
        "\n",
        "def installments_payments(data_dir='data', num_rows=None, nan_as_category=True):\n",
        "    ins = pd.read_csv(f'{data_dir}/installments_payments.csv', nrows=num_rows)\n",
        "    ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n",
        "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
        "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
        "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
        "    # Days past due and days before due (no negative values)\n",
        "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
        "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
        "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
        "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
        "    # Features: Perform aggregations\n",
        "    aggregations = {\n",
        "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
        "        'DPD': ['max', 'mean', 'sum'],\n",
        "        'DBD': ['max', 'mean', 'sum'],\n",
        "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
        "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
        "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
        "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
        "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
        "    }\n",
        "    for cat in cat_cols:\n",
        "        aggregations[cat] = ['mean']\n",
        "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
        "    ins_agg.columns = pd.Index(\n",
        "        ['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
        "    # Count installments accounts\n",
        "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
        "    del ins\n",
        "    gc.collect()\n",
        "    return ins_agg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-73oY8Z2h6W"
      },
      "source": [
        "### Preprocess credit_card_balance.csv\n",
        "\n",
        "#### credit_card_balance.csv\n",
        "\n",
        "Monthly balance snapshots of previous credit cards that the applicant has with Home Credit.\n",
        "\n",
        "This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has `(#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Fqvktcyy2iI9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def credit_card_balance(data_dir='data', num_rows=None, nan_as_category=True):\n",
        "    cc = pd.read_csv(f'{data_dir}/credit_card_balance.csv', nrows=num_rows)\n",
        "    cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n",
        "    # General aggregations\n",
        "    cc.drop(['SK_ID_PREV'], axis=1, inplace=True)\n",
        "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
        "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper()\n",
        "                              for e in cc_agg.columns.tolist()])\n",
        "    # Count credit card lines\n",
        "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
        "    del cc\n",
        "    gc.collect()\n",
        "    return cc_agg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iTDmtfq3POX"
      },
      "source": [
        "## 2.5 Preprocessing pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ojnUOSN93Paf"
      },
      "outputs": [],
      "source": [
        "def preprocess_pipeline(data_dir=DATA_FOLDER, num_rows=None, debug=False):\n",
        "    # num_rows = 10000 if debug else None\n",
        "    print(f'data_dir={data_dir}')\n",
        "    df = application_train(data_dir, num_rows)\n",
        "    start = time.time()\n",
        "    with timer(\"Process bureau and bureau_balance\"):\n",
        "        bureau = bureau_and_balance(data_dir, num_rows)\n",
        "        print(\"Bureau df shape:\", bureau.shape)\n",
        "        df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
        "        del bureau\n",
        "        gc.collect()\n",
        "    with timer(\"Process previous_applications\"):\n",
        "        prev = previous_applications(data_dir, num_rows)\n",
        "        print(\"Previous applications df shape:\", prev.shape)\n",
        "        df = df.join(prev, how='left', on='SK_ID_CURR')\n",
        "        del prev\n",
        "        gc.collect()\n",
        "    with timer(\"Process POS-CASH balance\"):\n",
        "        pos = pos_cash(data_dir, num_rows)\n",
        "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
        "        df = df.join(pos, how='left', on='SK_ID_CURR')\n",
        "        del pos\n",
        "        gc.collect()\n",
        "    with timer(\"Process installments payments\"):\n",
        "        ins = installments_payments(data_dir, num_rows)\n",
        "        print(\"Installments payments df shape:\", ins.shape)\n",
        "        df = df.join(ins, how='left', on='SK_ID_CURR')\n",
        "        del ins\n",
        "        gc.collect()\n",
        "    with timer(\"Process credit card balance\"):\n",
        "        cc = credit_card_balance(data_dir, num_rows)\n",
        "        print(\"Credit card balance df shape:\", cc.shape)\n",
        "        df = df.join(cc, how='left', on='SK_ID_CURR')\n",
        "        del cc\n",
        "        gc.collect()\n",
        "    time_taken = time.time() - start\n",
        "    print(f'preprocessing completed in {time_taken:.0f} s')\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfPMSbOd4GnL"
      },
      "source": [
        "### Test pre-processing pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJe_vR_Y3k90",
        "outputId": "9d092f3d-60dd-4d6c-aeda-af415993a1db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data_dir=../data/raw\n",
            "application_train, data_dir = ../data/raw\n",
            "Train samples: 307511\n",
            "Bureau df shape: (305811, 116)\n",
            "Process bureau and bureau_balance - done in 28s\n",
            "Previous applications df shape: (338857, 249)\n",
            "Process previous_applications - done in 29s\n",
            "Pos-cash balance df shape: (337252, 18)\n",
            "Process POS-CASH balance - done in 24s\n",
            "Installments payments df shape: (339587, 26)\n",
            "Process installments payments - done in 43s\n",
            "Credit card balance df shape: (103558, 141)\n",
            "Process credit card balance - done in 25s\n",
            "preprocessing completed in 149 s\n"
          ]
        }
      ],
      "source": [
        "df_data = preprocess_pipeline(data_dir=DATA_FOLDER, num_rows=None, debug=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "savepath = f'{OUT_FOLDER}/{CLEAN_DATA_FILENAME}'\n",
        "# with timer(f'Save to {savepath}'):\n",
        "# df_data.to_csv(savepath, encoding='UTF-8', sep='\\t', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(df_data) > SAMPLE_SIZE:\n",
        "    outpath = f'{OUT_FOLDER}/{CLEAN_DATA_SAMPLE}'\n",
        "    # with timer(f'Save sample (nb = {SAMPLE_SIZE}) to {outpath}'):\n",
        "    # df_data.sample(SAMPLE_SIZE).to_csv(\n",
        "    #     outpath, encoding='UTF-8', sep='\\t', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF-TKxkBBMC3"
      },
      "source": [
        "### Taille des fichiers nettoyés\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Huo2gSCr7Ya9",
        "outputId": "d703255e-730b-4b00-b0a2-aa458400b3d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "file : cleaned_data_scoring.csv : size=1006.01 Mo\n",
            "file : cleaned_data_sample.csv : size=327.74 Mo\n"
          ]
        }
      ],
      "source": [
        "for filename in [CLEAN_DATA_FILENAME, CLEAN_DATA_SAMPLE]:\n",
        "    filepath = f'{OUT_FOLDER}/{filename}'\n",
        "    print(f'file : {filename} : size={outils.io.get_filesize(filepath)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "P7_nettoyage.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "9f161018c5e2fdb25091822214a6434325c4d2222b4c3ef81019bf1117201605"
    },
    "kernelspec": {
      "display_name": "Python 3.7.0 ('OC_3')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
